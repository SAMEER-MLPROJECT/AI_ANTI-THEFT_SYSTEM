import pandas as pd
import numpy as np
import time
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')


data = pd.read_csv('consumption_ledger.csv')


def prepare_data(data, sequence_length=30):
    scaler = MinMaxScaler(feature_range=(0, 1))
    data['normalized'] = scaler.fit_transform(data[['consumption']])
    
    
    X, y = [], []
    for i in range(sequence_length, len(data['normalized'])):
        X.append(data['normalized'][i-sequence_length:i].values)
        y.append(data['normalized'][i])
    X, y = np.array(X), np.array(y)
    
    return X, y, scaler


def train_isolation_forest(data):
    params = {'n_estimators': [100, 150], 'max_samples': ['auto'], 'contamination': [0.01, 0.02]}
    if_model = GridSearchCV(IsolationForest(), params, cv=3, scoring='accuracy', n_jobs=-1)
    if_model.fit(data[['consumption']])
    return if_model.best_estimator_


def train_lstm(X, y):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(X.shape[1], 1)),
        LSTM(32),
        Dense(1)
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model.fit(X, y, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stop], verbose=1)
    return model


X, y, scaler = prepare_data(data)
lstm_model = train_lstm(X, y)
isolation_forest_model = train_isolation_forest(data)


def detect_anomalies(new_data, if_model, lstm_model, scaler, sequence_length=30):
   
    new_data['normalized'] = scaler.transform(new_data[['consumption']])
    
    
    anomaly_flag = if_model.predict(new_data[['consumption']].values)
    new_data['iforest_anomaly'] = anomaly_flag == -1
    
   
    last_sequence = new_data['normalized'].values[-sequence_length:].reshape(1, sequence_length, 1)
    predicted = lstm_model.predict(last_sequence)
    predicted_value = scaler.inverse_transform(predicted)
    
    
    deviation = abs(predicted_value - new_data['consumption'].values[-1])
    threshold = 0.1 * new_data['consumption'].values[-1]  # Dynamic threshold
    lstm_anomaly = deviation > threshold
    
    return {
        'iforest_anomaly': new_data['iforest_anomaly'].iloc[-1],
        'lstm_anomaly': lstm_anomaly[0][0],
        'predicted_consumption': predicted_value[0][0]
    }


def log_anomalies(anomaly_data):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open('anomaly_log.txt', 'a') as log_file:
        log_file.write(f"{timestamp} - Anomaly detected: {anomaly_data}\n")
    print(f"[ALERT] {timestamp} - Anomaly detected:", anomaly_data)


def main_monitoring_loop():
    global isolation_forest_model, lstm_model, scaler
    retrain_count = 0
    while True:
        new_data = pd.DataFrame({'consumption': [np.random.uniform(20, 100)]})
        
       
        anomalies = detect_anomalies(new_data, isolation_forest_model, lstm_model, scaler)
        
       
        if anomalies['iforest_anomaly'] or anomalies['lstm_anomaly']:
            log_anomalies(anomalies)
        
        
        retrain_count += 1
        if retrain_count % 100 == 0:
            print("Retraining models with updated data...")
            # Append new_data to the historical dataset and retrain models
            data.append(new_data)
            X, y, scaler = prepare_data(data)
            lstm_model = train_lstm(X, y)
            isolation_forest_model = train_isolation_forest(data)
            print("Models retrained.")
        
        time.sleep(2)  


main_monitoring_loop()
